{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differantial Private Deep Learning\n",
    "\n",
    "When our neural network are learning from sensitive data, that thay are only learning what they're supposed to learn from the data without accidentally learning they are not supposed to learn from the data.\n",
    "\n",
    "The general goal of differantial privacy is to ensure that different kind of statistical analysis do not compromise the privacy.\n",
    "\n",
    "The defination of Privacy: You will not affected, adversly on otherwise, by allowing your data to be used in any study on analysis, no matter what other studies,datasets, orinformation sources, are avaible.\n",
    "\n",
    "\n",
    "Exercise\n",
    "* Step one is to create our database - we're going to do this by initializing a random list of 1s and 0s (which are the entries in our database). Note - the number of entries directly corresponds to the number of people in our database.\n",
    "\n",
    "*  I want you to create a list of every parallel database to the one currently contained in the \"db\" variable. Then, I want you to create a function which both:\n",
    "\n",
    "    * creates the initial database (db)\n",
    "    * creates all parallel databases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to crete a databse\n",
    "import torch\n",
    "def remove_index(db,index):\n",
    "    \"\"\"Remove the defined index from database\"\"\"\n",
    "    #removing the index\n",
    "    return torch.cat((db[0:index],db[index + 1:]))\n",
    "\n",
    "def paralle_db(db):\n",
    "    \"\"\"Create paralel data base for a given database with using remove_index function\"\"\"\n",
    "    #create the empty list for paralel databases\n",
    "    paralel_dbs = list()\n",
    "    #loop over the database\n",
    "    for i in range(len(db)):\n",
    "        #create the database for each removed index\n",
    "        dbs = remove_index(db,i)\n",
    "        #appande the removed databases to list of paralel databases\n",
    "        paralel_dbs.append(dbs)\n",
    "    return paralel_dbs\n",
    "\n",
    "def create_and_paralel(num_enrty):\n",
    "    \"\"\"Create simple database for a single column and then call the parael_db to\n",
    "    create paralal databases for each removed index value\"\"\"\n",
    "    #create the simple database for specific number \n",
    "    db = torch.rand(num_enrty) > 0.5\n",
    "    #call paralle_bd function to create paralal databases for spefic number of element\n",
    "    pbds = paralle_db(db)\n",
    "    return db, pbds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd, pdbs = create_and_paralel(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 0, 1,  ..., 0, 0, 0], dtype=torch.uint8), tensor([1, 0, 1,  ..., 0, 0, 0], dtype=torch.uint8), tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.uint8), tensor([1, 1, 0,  ..., 0, 0, 0], dtype=torch.uint8), tensor([1, 1, 0,  ..., 0, 0, 0], dtype=torch.uint8)]\n"
     ]
    }
   ],
   "source": [
    "#let's check out first 6 tenser list from pdbs\n",
    "print(pdbs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the The Privacy of the Function\n",
    "\n",
    "*  How to query this database\n",
    "* then measure the privacy of that query\n",
    "\n",
    "We are going to compare the output of the query on the entire database, with the output of the query on each of the paralel databases. That way, we are going to how the query change, when we remove an individual from tha database.\n",
    "\n",
    "* We are going to query the full database\n",
    "* Weare going to look at quering every possible paralel database\n",
    "* What's the maxmimum around that they different?\n",
    "\n",
    "When we remove the people from this database, it changes the output of query. the output of this queryis actually conditionied directy on information from a lot of people in this databse.\n",
    "\n",
    "Sensitivity: The maximum amount that the query changes when removing an individual from the database. it is called L1 sensitivity for simply.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(db):\n",
    "    return db.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db, pdbs = create_and_paralel(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_db_result = query(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivy = 0\n",
    "for dbs in pdbs:\n",
    "    pdb_result = query(dbs)\n",
    "    db_distance = torch.abs(pdb_result - full_db_result)\n",
    "    \n",
    "    if (db_distance > sensitivy):\n",
    "        sensitivy = db_distance\n",
    "    \n",
    "sensitivy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(db):\n",
    "    return db.float().mean()\n",
    "\n",
    "def sensitivity(query, n_entries =1000):\n",
    "    db, pdbs = create_and_paralel(n_entries)\n",
    "   \n",
    "    full_db_result = query(db)\n",
    "    sensitivity = 0\n",
    "    for pdb in pdbs:\n",
    "        pdb_result = query(pdb)\n",
    "\n",
    "        db_distance = torch.abs(pdb_result - full_db_result)\n",
    "\n",
    "        if(db_distance > sensitivity):\n",
    "            sensitivity = db_distance\n",
    "    \n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0005)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local & Global Differantial Privacy\n",
    "**Local differantial privacy:** Add noise to the function dat points\n",
    "**Global Differantial pricacy:** Add noise on the output query on the database\n",
    "\n",
    "## Local Differantial Privacy\n",
    "\n",
    "Where given a collection of individual, each individial adds noise to the their data before sending it to the statistical database itself. So, everything that gets through is alrready noised. So, the protection is happening at the local level. \n",
    "\n",
    "Differantial privacy always requires a form of randomness or noise added to the quey to protec from like differnatial attack.\n",
    "\n",
    "**Ransomized Responce:** Techniquie that is used social sciences when tring to a learn about the high level trends for a taban behavior.\n",
    "\n",
    "Let's say I have a group of people I wish to survey about a very taboo behavior which I think they will lie about (say, I want to know if they have ever committed a certain kind of crime). I'm not a policeman, I'm just trying to collect statistics to understand the higher level trend in society. So, how do we do this? One technique is to add randomness to each person's response by giving each person the following instructions (assuming I'm asking a simple yes/no question):\n",
    "\n",
    "**Flip a coin 2 times.**\n",
    "* If the first coin flip is heads, answer honestly\n",
    "* If the first coin flip is tails, answer according to the second coin flip (heads for yes, tails for no)!\n",
    "\n",
    "Thus, each person is now protected with \"plausible deniability\". If they answer \"Yes\" to the question \"have you committed X crime?\", then it might becasue they actually did, or it might be becasue they are answering according to a random coin flip. Each person has a high degree of protection. Furthermore, we can recover the underlying statistics with some accuracy, as the \"true statistics\" are simply averaged with a 50% probability. Thus, if we collect a bunch of samples and it turns out that 60% of people answer yes, then we know that the TRUE distribution is actually centered around 70%, because 70% averaged wtih 50% (a coin flip) is 60% which is the result we obtained.\n",
    "\n",
    "However, it should be noted that, especially when we only have a few samples, the this comes at the cost of accuracy. This tradeoff exists across all of Differential Privacy. The greater the privacy protection (plausible deniability) the less accurate the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6200)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db, pdbs = create_and_paralel(100)\n",
    "#db\n",
    "first_coin_flip = (torch.rand(len(db))> 0.5).float()\n",
    "#first_coin_flip\n",
    "second_coin_flip = (torch.rand(len(db))> 0.5).float()\n",
    "augmented_databse = (db.float() * first_coin_flip) + ((1-first_coin_flip) * second_coin_flip)\n",
    "augmented_databse\n",
    "torch.mean(augmented_databse).float() *2 -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**in the video**\n",
    "\n",
    "$augmented\\_databse\\_mean = (true\\_mean *noise) + (noise\\_dist\\_mean*(1-noise))$\n",
    "\n",
    "**it is also equel to**\n",
    "\n",
    "$augmented\\_databse\\_mean = (true\\_mean *(1-noise)) + (noise\\_dist\\_mean* noise)$\n",
    "\n",
    "**changing the $noise$ and $(1-noise)$ are the same...**\n",
    "\n",
    "So, if we try to get $true\\_mean$ here\n",
    "\n",
    "**Note: $augmented\\_databse\\_mean is alse called sk_result which we are calculating before. So, I am going to continue with sk_result**\n",
    "\n",
    "$ sk\\_result =(true\\_mean *(1-noise)) + (noise\\_dist\\_mean* noise)$\n",
    "\n",
    "we want to $true\\_mean$ alone. So, I want to take (1-noise) paranteses for both term\n",
    "\n",
    "$sk\\_result = (1-noise)(true\\_mean + noise\\_dist\\_mean* (\\frac{noise}{1-noise}) )$\n",
    "\n",
    "$ \\frac{sk\\_result}{(1-noise)} =  true\\_mean + noise\\_dist\\_mean * (\\frac{noise}{1-noise})$\n",
    "\n",
    " $true\\_mean = \\frac{sk\\_result}{(1-noise)} - noise\\_dist\\_mean * (\\frac{noise}{1-noise}) $\n",
    " \n",
    "**S0, we can take the $\\frac{noise}{(1-noise)}$ parantehesis**\n",
    "\n",
    "$true\\_mean = (\\frac{noise}{(1-noise)}) * (\\frac{sk\\_result}{noise} - noise\\_dist\\_mean)  $\n",
    "\n",
    "we know that $noise\\_dist\\_mean = 0.5$\n",
    "\n",
    "$true\\_mean = (\\frac{noise}{(1-noise)}) * (\\frac{sk\\_result}{noise} - 0.5)  $\n",
    "\n",
    "$true\\_mean = ((sk\\_learn / noise) - 0.5)* noise/(1-noise) $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
