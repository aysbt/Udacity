{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differantial Private Deep Learning\n",
    "\n",
    "When our neural network are learning from sensitive data, that thay are only learning what they're supposed to learn from the data without accidentally learning they are not supposed to learn from the data.\n",
    "\n",
    "The general goal of differantial privacy is to ensure that different kind of statistical analysis do not compromise the privacy.\n",
    "\n",
    "The defination of Privacy: You will not affected, adversly on otherwise, by allowing your data to be used in any study on analysis, no matter what other studies,datasets, orinformation sources, are avaible.\n",
    "\n",
    "\n",
    "Exercise\n",
    "* Step one is to create our database - we're going to do this by initializing a random list of 1s and 0s (which are the entries in our database). Note - the number of entries directly corresponds to the number of people in our database.\n",
    "\n",
    "*  I want you to create a list of every parallel database to the one currently contained in the \"db\" variable. Then, I want you to create a function which both:\n",
    "\n",
    "    * creates the initial database (db)\n",
    "    * creates all parallel databases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to crete a databse\n",
    "import torch\n",
    "def remove_index(db,index):\n",
    "    \"\"\"Remove the defined index from database\"\"\"\n",
    "    #removing the index\n",
    "    return torch.cat((db[0:index],db[index + 1:]))\n",
    "\n",
    "def paralle_db(db):\n",
    "    \"\"\"Create paralel data base for a given database with using remove_index function\"\"\"\n",
    "    #create the empty list for paralel databases\n",
    "    paralel_dbs = list()\n",
    "    #loop over the database\n",
    "    for i in range(len(db)):\n",
    "        #create the database for each removed index\n",
    "        dbs = remove_index(db,i)\n",
    "        #appande the removed databases to list of paralel databases\n",
    "        paralel_dbs.append(dbs)\n",
    "    return paralel_dbs\n",
    "\n",
    "def create_and_paralel(num_enrty):\n",
    "    \"\"\"Create simple database for a single column and then call the parael_db to\n",
    "    create paralal databases for each removed index value\"\"\"\n",
    "    #create the simple database for specific number \n",
    "    db = torch.rand(num_enrty) > 0.5\n",
    "    #call paralle_bd function to create paralal databases for spefic number of element\n",
    "    pbds = paralle_db(db)\n",
    "    return db, pbds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd, pdbs = create_and_paralel(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1, 0, 1,  ..., 0, 0, 0], dtype=torch.uint8), tensor([1, 0, 1,  ..., 0, 0, 0], dtype=torch.uint8), tensor([1, 1, 1,  ..., 0, 0, 0], dtype=torch.uint8), tensor([1, 1, 0,  ..., 0, 0, 0], dtype=torch.uint8), tensor([1, 1, 0,  ..., 0, 0, 0], dtype=torch.uint8)]\n"
     ]
    }
   ],
   "source": [
    "#let's check out first 6 tenser list from pdbs\n",
    "print(pdbs[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the The Privacy of the Function\n",
    "\n",
    "*  How to query this database\n",
    "* then measure the privacy of that query\n",
    "\n",
    "We are going to compare the output of the query on the entire database, with the output of the query on each of the paralel databases. That way, we are going to how the query change, when we remove an individual from tha database.\n",
    "\n",
    "* We are going to query the full database\n",
    "* Weare going to look at quering every possible paralel database\n",
    "* What's the maxmimum around that they different?\n",
    "\n",
    "When we remove the people from this database, it changes the output of query. the output of this queryis actually conditionied directy on information from a lot of people in this databse.\n",
    "\n",
    "Sensitivity: The maximum amount that the query changes when removing an individual from the database. it is called L1 sensitivity for simply.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(db):\n",
    "    return db.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db, pdbs = create_and_paralel(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_db_result = query(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivy = 0\n",
    "for dbs in pdbs:\n",
    "    pdb_result = query(dbs)\n",
    "    db_distance = torch.abs(pdb_result - full_db_result)\n",
    "    \n",
    "    if (db_distance > sensitivy):\n",
    "        sensitivy = db_distance\n",
    "    \n",
    "sensitivy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(db):\n",
    "    return db.float().mean()\n",
    "\n",
    "def sensitivity(query, n_entries =1000):\n",
    "    db, pdbs = create_and_paralel(n_entries)\n",
    "   \n",
    "    full_db_result = query(db)\n",
    "    sensitivity = 0\n",
    "    for pdb in pdbs:\n",
    "        pdb_result = query(pdb)\n",
    "\n",
    "        db_distance = torch.abs(pdb_result - full_db_result)\n",
    "\n",
    "        if(db_distance > sensitivity):\n",
    "            sensitivity = db_distance\n",
    "    \n",
    "    return sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0005)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local & Global Differantial Privacy\n",
    "**Local differantial privacy:** Add noise to the function dat points\n",
    "**Global Differantial pricacy:** Add noise on the output query on the database\n",
    "\n",
    "## Local Differantial Privacy\n",
    "\n",
    "Where given a collection of individual, each individial adds noise to the their data before sending it to the statistical database itself. So, everything that gets through is alrready noised. So, the protection is happening at the local level. \n",
    "\n",
    "Differantial privacy always requires a form of randomness or noise added to the quey to protec from like differnatial attack.\n",
    "\n",
    "**Ransomized Responce:** Techniquie that is used social sciences when tring to a learn about the high level trends for a taban behavior.\n",
    "\n",
    "Let's say I have a group of people I wish to survey about a very taboo behavior which I think they will lie about (say, I want to know if they have ever committed a certain kind of crime). I'm not a policeman, I'm just trying to collect statistics to understand the higher level trend in society. So, how do we do this? One technique is to add randomness to each person's response by giving each person the following instructions (assuming I'm asking a simple yes/no question):\n",
    "\n",
    "**Flip a coin 2 times.**\n",
    "* If the first coin flip is heads, answer honestly\n",
    "* If the first coin flip is tails, answer according to the second coin flip (heads for yes, tails for no)!\n",
    "\n",
    "Thus, each person is now protected with \"plausible deniability\". If they answer \"Yes\" to the question \"have you committed X crime?\", then it might becasue they actually did, or it might be becasue they are answering according to a random coin flip. Each person has a high degree of protection. Furthermore, we can recover the underlying statistics with some accuracy, as the \"true statistics\" are simply averaged with a 50% probability. Thus, if we collect a bunch of samples and it turns out that 60% of people answer yes, then we know that the TRUE distribution is actually centered around 70%, because 70% averaged wtih 50% (a coin flip) is 60% which is the result we obtained.\n",
    "\n",
    "However, it should be noted that, especially when we only have a few samples, the this comes at the cost of accuracy. This tradeoff exists across all of Differential Privacy. The greater the privacy protection (plausible deniability) the less accurate the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6200)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db, pdbs = create_and_paralel(100)\n",
    "#db\n",
    "first_coin_flip = (torch.rand(len(db))> 0.5).float()\n",
    "#first_coin_flip\n",
    "second_coin_flip = (torch.rand(len(db))> 0.5).float()\n",
    "augmented_databse = (db.float() * first_coin_flip) + ((1-first_coin_flip) * second_coin_flip)\n",
    "augmented_databse\n",
    "torch.mean(augmented_databse).float() *2 -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**in the video**\n",
    "\n",
    "$$augmented\\_databse\\_mean = (true\\_mean *noise) + (noise\\_dist\\_mean*(1-noise))$$\n",
    "\n",
    "**it is also equel to**\n",
    "\n",
    "$$augmented\\_databse\\_mean = (true\\_mean *(1-noise)) + (noise\\_dist\\_mean* noise)$$\n",
    "\n",
    "**changing the $noise$ and $(1-noise)$ are the same...**\n",
    "\n",
    "So, if we try to get $true\\_mean$ here\n",
    "\n",
    "**Note: $augmented\\_databse\\_mean is alse called sk_result which we are calculating before. So, I am going to continue with sk_result**\n",
    "\n",
    "$$ sk\\_result =(true\\_mean *(1-noise)) + (noise\\_dist\\_mean* noise)$$\n",
    "\n",
    "we want to $true\\_mean$ alone. So, I want to take (1-noise) paranteses for both term\n",
    "\n",
    "$$sk\\_result = (1-noise)(true\\_mean + noise\\_dist\\_mean* (\\frac{noise}{1-noise}) )$$\n",
    "\n",
    "$$ \\frac{sk\\_result}{(1-noise)} =  true\\_mean + noise\\_dist\\_mean * (\\frac{noise}{1-noise})$$\n",
    "\n",
    " \n",
    " $$true\\_mean = \\frac{sk\\_result}{(1-noise)} - noise\\_dist\\_mean * (\\frac{noise}{1-noise}) $$\n",
    " \n",
    "**S0, we can take the $\\frac{noise}{(1-noise)}$ parantehesis**\n",
    "\n",
    "$$true\\_mean = (\\frac{noise}{(1-noise)}) * (\\frac{sk\\_result}{noise} - noise\\_dist\\_mean)  $$\n",
    "\n",
    "we know that $noise\\_dist\\_mean = 0.5$\n",
    "\n",
    "$$true\\_mean = (\\frac{noise}{(1-noise)}) * (\\frac{sk\\_result}{noise} - 0.5)  $$\n",
    "\n",
    "$$true\\_mean = ((sk\\_learn / noise) - 0.5)* noise/(1-noise) $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryy(db):\n",
    "    true_mean = db.float().mean()\n",
    "    #flip the coins\n",
    "    first_coin_flip = (torch.rand(len(db)) > 0.5).float()\n",
    "    second_coin_flip = (torch.rand(len(db)) > 0.5).float()\n",
    "    #augmented_databse \n",
    "    augmented_databse = (first_coin_flip * db.float()) + ((1 - first_coin_flip) * second_coin_flip) \n",
    "    augmented_databse_mean = augmented_databse.float().mean()\n",
    "    return true_mean, augmented_databse_mean\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Noise : tensor(0.4720)\n",
      "Witout Noise: tensor(0.4640)\n"
     ]
    }
   ],
   "source": [
    "db, pdbs =create_and_paralel(1000)\n",
    "true_mean, augmented_databse_mean =  queryy(db)\n",
    "print(\"With Noise : \" +str(true_mean))\n",
    "print(\"Witout Noise: \"+ str(augmented_databse_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_noise(db, noise=0.2):\n",
    "    true_mean = db.float().mean()\n",
    "    #flip the coins\n",
    "    first_coin_flip = (torch.rand(len(db)) > noise).float()\n",
    "    second_coin_flip = (torch.rand(len(db)) > 0.5).float()\n",
    "    #augmented_databse \n",
    "    augmented_databse = (first_coin_flip * db.float()) + ((1 - first_coin_flip) * second_coin_flip) \n",
    "    sk_result = augmented_databse.float().mean()\n",
    "    private_result = ((sk_result/noise)- 0.5)*( noise*(1-noise))\n",
    "    return sk_result, private_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Noise : tensor(0.5110)\n",
      "Witout Noise: tensor(0.3288)\n"
     ]
    }
   ],
   "source": [
    "db, pdbs =create_and_paralel(1000)\n",
    "true_mean, augmented_databse_mean =  query_noise(db, noise=0.2)\n",
    "print(\"With Noise : \" +str(true_mean))\n",
    "print(\"Witout Noise: \"+ str(augmented_databse_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Formal Definition of Differential Privacy\n",
    "How much noise should add in the query?\n",
    "\n",
    "we are going to create a database with our quey then analyze it with the formal  defination of differential privacy:\n",
    "```python\n",
    "def query(db, epsilon=1):\n",
    "    scale = 1/epsilon\n",
    "    s = np.random.laplace(0, scale, 1)\n",
    "    #adding the privacy code outside og sum\n",
    "    return (torch.sum(db.float()) + s)[0]\n",
    "   \n",
    "```\n",
    "We are going to add noise to our function and we have two differant kinds of noise that we can add, Laplacian noise, Gaussian noise,\n",
    "\n",
    "The formula is a contraint so that you can analyze a query with noise and know whether or not this query is leaking too much information.\n",
    "\n",
    "Global differantial privacy  instead of adding noise to individual data points applies noise to the output of a function on data points. \n",
    "\n",
    "Advantage of this, you can often add a lot less noise and get a much more accurate result if youwait to add the noise until after you are  computed a function.\n",
    "\n",
    "\n",
    "How do we actually measure how much privacy is being leaked inside of a diffferantially private algorithm?\n",
    "\n",
    "\n",
    "**Differantial Privacy:** \n",
    "\n",
    "A normalized algorithm $\\mathcal{M}$ with domain $\\mathbb{N}^{|x|}$ is ($\\epsilon, \\, \\delta$)- differantially private if for all $\\mathcal{S}\\subseteq$ Range($\\mathcal{M}$) and for all $x,y \\in \\mathbb{N}^{|x|}$ such that $||x-y||_1 \\le 1$\n",
    "\n",
    "$$Pr[\\mathcal{M}(x) \\in \\mathcal{S}] \\le exp(\\epsilon) Pr[\\mathcal{M}(y) \\in \\mathcal{S}] + \\delta$$\n",
    "\n",
    "\n",
    "M being a randomized is:\n",
    "```python\n",
    "def M(db):\n",
    "    return query(db) +noise \n",
    "```\n",
    "It's some  sort of function on a database with some sort of noise added to it. This would be globally diffrential private algorithm (query(db) +noise) or this could have been inside the database query(db+noise) which makes is local differantial private. So, where the noise is actually added is does not matter, the exact mechanism is nonspecific in the formula. So, this could be any function, we just know that it has the ability to query a databse and makes randomized in some way. And it has a output domain $\\mathcal{N}(x)$, natural numbers, meaning it could be a histogram over certain entries in the database . So, it's discretized in this particular case, and we're saying that this is epsilon delta differantially private if for all the potential things that $\\mathcal{M}$  could predic, so, all potentiall outputs of $\\mathcal{M}$ for all potential inputs(x,y) they are paralel databases. Menaing he $Pr[\\mathcal{M}(x) \\in \\mathcal{S}] \\le exp(\\epsilon) Pr[\\mathcal{M}(y) \\in \\mathcal{S}] + \\delta$ equation is TRUE. \n",
    "\n",
    "Actually,$x,y$ are histograms over pair databases, and this two histograms maximum ditance could be 1, meaning they have jush have one different entries in their databases.\n",
    "\n",
    "$$Pr[\\mathcal{M}(x) \\in \\mathcal{S}] \\le exp(\\epsilon) Pr[\\mathcal{M}(y) \\in \\mathcal{S}] + \\delta$$\n",
    "\n",
    "x. takes our mechanism and running it over histogram and y is doing the same thing for any paralel database with one entry missing. So, this threshold has to be true for all databeses and its returning something in $\\mathcal{S}$, S is one of possible thing that it could be predicting in the range of $\\mathcal{M}$, \n",
    "\n",
    "The core here how different are these two distribution is. How much does it changes is measured by two prameters, $\\epsilon, \\delta$. $exp(\\epsilon)$ is show how diffrent these two distrubutin is, \n",
    "\n",
    "Epsilon Zero: If a query satisfied this inequality where epsilon was set to 0, then that would mean that the query for all parallel databases outputed the exact same value as the full database. As you may remember, when we calculated the \"threshold\" function, often the Sensitivity was 0. In that case, the epsilon also happened to be zero.\n",
    "\n",
    "Epsilon One: If a query satisfied this inequality with epsilon 1, then the maximum distance between all queries would be 1 - or more precisely - the maximum distance between the two random distributions M(x) and M(y) is 1 (because all these queries have some amount of randomness in them, just like we observed in the last section).\n",
    "\n",
    "\n",
    "\n",
    "Now, $\\delta$: is the probabbilty that you will accidenty leak, more information then $\\epsilon$ claims that you will leak. it is usually very small number\n",
    "\n",
    "# Lesson: How To Add Noise for Global Differential Privacy\n",
    "\n",
    "In this lesson, we're going to learn about how to take a query and add varying amounts of noise so that it satisfies a certain degree of differential privacy. In particular, we're going to leave behind the Local Differential privacy previously discussed and instead opt to focus on Global differential privacy. \n",
    "\n",
    "So, to sum up, this lesson is about adding noise to the output of our query so that it satisfies a certain epsilon-delta differential privacy threshold.\n",
    "\n",
    "There are two kinds of noise we can add - Gaussian Noise or Laplacian Noise. Generally speaking Laplacian is better, but both are still valid. Now to the hard question...\n",
    "\n",
    "### How much noise should we add?\n",
    "\n",
    "The amount of noise necessary to add to the output of a query is a function of four things:\n",
    "\n",
    "- the type of noise (Gaussian/Laplacian)\n",
    "- the sensitivity of the query/function\n",
    "- the desired epsilon (ε)\n",
    "- the desired delta (δ)\n",
    "\n",
    "Thus, for each type of noise we're adding, we have different way of calculating how much to add as a function of sensitivity, epsilon, and delta. We're going to focus on Laplacian noise. Laplacian noise is increased/decreased according to a \"scale\" parameter b. We choose \"b\" based on the following formula.\n",
    "\n",
    "b = sensitivity(query) / epsilon\n",
    "\n",
    "In other words, if we set b to be this value, then we know that we will have a privacy leakage of <= epsilon. Furthermore, the nice thing about Laplace is that it guarantees this with delta == 0. There are some tunings where we can have very low epsilon where delta is non-zero, but we'll ignore them for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_query_sum(db, epsilon=1):\n",
    "    #b = sensitiviyu/epsilon\n",
    "    scale = 1/ epsilon\n",
    "    s = torch.tensor(np.random.laplace(0, scale,1))\n",
    "    return (torch.sum(db.float()) + s)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "db, pdbs = create_and_paralel(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42.)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#true results\n",
    "torch.sum(db.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([42.5033], dtype=torch.float64)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_laplace_query = laplace_query_sum(db, epsilon=1)\n",
    "sum_laplace_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
