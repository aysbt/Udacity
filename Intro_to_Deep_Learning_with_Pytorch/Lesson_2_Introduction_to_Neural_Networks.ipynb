{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Into to Deep learning with PyTorch\n",
    "\n",
    "# Lesson 2: Introduction to Neural Networks\n",
    "One neuron canâ€™t do much on its own. Usually we will have many neurons labelled by indices k, i, j and activation flows between them via synapses with strengths wki, wij:\n",
    "\n",
    "![title](../../../Documents/screees_shoot/Screen Shot 2019-05-07 at 2.26.58 PM.png)\n",
    "\n",
    "### Perception Algoritm\n",
    "\n",
    "* start with random weights $w_1$ ...... $w_n$ .... b\n",
    "* For every misclassfred point $(x_1 - x_n):$\n",
    "    * if prediction is 0:\n",
    "        * For i =1 .... n \n",
    "            * change $w_i + \\alpha x_i$\n",
    "        * change b to b + $\\alpha$ \n",
    "    * if prediction is 1:\n",
    "        * for i=1 .... n\n",
    "            * change $w_i - \\alpha x_i$ \n",
    "        * change b to b - $\\alpha$ \n",
    "\n",
    "\n",
    "Recall that the perceptron step works as follows. For a point with coordinates $(p,q)(p,q)$, label yy, and prediction given by the equation \n",
    "\n",
    "\n",
    "$$y^\\^ = step(w_1 x_1 + w_2 x_2 + b)$$\n",
    "\n",
    "\n",
    "\n",
    "* If the point is correctly classified, do nothing.\n",
    "* If the point is classified positive, but it has a negative label, subtract $\\alpha p$,$ \\alpha q$, and $\\alpha$ from $w_1$, $w_2$,$b$ respectively.\n",
    "* If the point is classified negative, but it has a positive label, add $\\alpha p$, $\\alpha q$, and $\\alpha$ to $w_1$, $w_2$,and $b$ respectively.\n",
    "\n",
    "\n",
    "if we want to get separete the non-linear region, we need to use the error function. error function is simply something that tells us how for we are from the solution. \n",
    "\n",
    "Prediction is basically the answer of algoritm. A discreet answer will be the \"yes\" or \"no\". Whereas, continued answer will be number between 0 and 1.\n",
    "\n",
    "the way we move from discreate to continuous, is to simply change your activation function from the step function to **sigmoid function**. The sigmoid function is simply a functin which for large positive numbers will give us values very close to one. for large negative number will give us values close to zero. it'll give you values that are close to zero. it will gives us the probability 0.5.\n",
    "\n",
    "the sigmoid function: \n",
    "$$\\sigma(x)= \\frac{1}{1 +e^{-x}} $$\n",
    "\n",
    "we just combine the linear $wx +b$ function with sigmoid function. it takes the inputs, multiplies them by the weight in the edge  and adds the results then applies the sigmoid function. it returns the values between 0 and 1. \n",
    "\n",
    "### Multi-Class and Softmax function\n",
    "\n",
    "Where the problem has 3 or more clases we are going to use the softmax function.\n",
    "\n",
    "**exponantinal function return every number into positive number**\n",
    "\n",
    "**Softmax function:**\n",
    " Linear function scores $Z_1, Z_2, ..... Z_n$ each score dor each of the classes \n",
    " \n",
    " $$P(class_i) = \\frac{e^{z_i}}{e^{z_1}+ ..... e^{z_n}}$$\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#define the list of score for each class\n",
    "L=[5,6,7]\n",
    "def sofmax(L):\n",
    "    exp = np.exp(L)\n",
    "    return np.divide(exp, sum(exp))\n",
    "#this are probbality of for given class score\n",
    "print(sofmax(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood \n",
    "We pick the model that gives the exisiting labels the highest probability.\n",
    "\n",
    "* Better model will gives us better probability\n",
    "* Minimaxing the error function will take us to the best possible solution.\n",
    "* $$log(ab) = lob(a)+log(b)$$\n",
    "    * lograritm of a number between 0 and 1 is always negative number\n",
    "    * take the negative of the algoritm of the probabilies\n",
    "    * sum the negatives of the probabilities which calles **croos entropy**\n",
    "* Good model will gives us low cross entropy and bad model will give us a high cross entropy\n",
    "* Correctly classified point will have a probability that cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
